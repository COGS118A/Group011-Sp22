{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A- Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "You will design and execute a machine learning project. There are a few constraints on the nature of the allowed project. \n",
    "- The problem addressed will not be a \"toy problem\" or \"common training students problem\" like mtcars, iris, palmer penguins etc.\n",
    "- The dataset will have >1k observations and >5 variables. I'd prefer more like >10k observations and >10 variables. A general rule is that if you have >100x more observations than variables, your solution will likely generalize a lot better. The goal of training a supervised machine learning model is to learn the underlying pattern in a dataset in order to generalize well to unseen data, so choosing a large dataset is very important.\n",
    "\n",
    "- The project will include a model selection and/or feature selection component where you will be looking for the best setup to maximize the performance of your ML system.\n",
    "- You will evaluate the performance of your ML system using more than one appropriate metric\n",
    "- You will be writing a report describing and discussing these accomplishments\n",
    "\n",
    "\n",
    "Feel free to delete this description section when you hand in your proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peer Review\n",
    "\n",
    "You will all have an opportunity to look at the Project Proposals of other groups to fuel your creativity and get more ideas for how you can improve your own projects. \n",
    "\n",
    "Both the project proposal and project checkpoint will have peer review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "Hopefully your team is at least this good. Obviously you should replace these with your names.\n",
    "\n",
    "- Dongze Li\n",
    "- Feifan Li\n",
    "- Rosy Xu\n",
    "- Shaolong Li\n",
    "- Zikang Chen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Nowadays, heart disease can be regarded as one of the most fatal diseases in the world. And nearly 647,000 Americans die from heart disease every year. Thus, developing an ML model to detect and predict heart disease may help the patients get effective treatment at the early stage, significantly reducing the probability of causing death. To build the model, we decide to use a dataset that measures the possible indicators that cause heart disease like high blood pressure, alcohol, or smoking. By evaluating these indicators, we may be able to predict whether an individual has heart disease. CDC collects the data in the dataset through telephone surveys, including the health status of US adults in over 50 states. And before building the ML model, we will firstly preprocess the data, converting them into appropriate data types and abandoning some features if needed. After the model is created, we will use some metrics or criteria to evaluate its accuracy, determining whether it is a successful model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Heart disease is one of the leading causes of death for all people from all racial and ethnic groups in the United States. According to the data from CDC, “one person dies every 36 seconds in the United States from cardiovascular disease<a name=\"CDC\"></a>[<sup>[1]</sup>](#CDC) and about 659,000 people in the United States die from heart disease each year<a name=\"CDC\"></a>[<sup>[1]</sup>](#CDC), which is 1 in every 4 deaths<a name=\"CDC\"></a>[<sup>[1]</sup>](#CDC)”. Besides, heart disease placed huge fiscal stress on the Federal government, “costing about 363 billion dollars each year from 2016 to 2017\"<a name=\"CDC\"></a>[<sup>[1]</sup>](#CDC).\n",
    "\n",
    "Many prior works have been done in this area. One prior work done by Davide Chicco & Giuseppe Jurman<a name=\"BMC\"></a>[<sup>[2]</sup>](#BMC) analyzed a dataset of 299 patients with heart failure and performed an alternative feature ranking analysis using traditional biostatistics tests. \n",
    "\n",
    "Another work done by Rohit Bharti, Aditya Khamparia and et al.<a name=\"Hindawi\"></a>[<sup>[3]</sup>](#Hindawi) tried to predict Heart Disease using a combination of Machine Learning and Deep Learning. They used a Public Health Dataset containing 76 attributes and employed various methods including KNN, PCA and SVM to do the analysis. \n",
    "\n",
    "Using ML algorithms to do the Heart Disease Prediction can be really important because good prediction can help lower the risk of the disease and further prevent life threat. Even an inaccurate prediction can indirectly provide us with some meaningful data that shows relationship between the factors that contribute most to heart disease. Therefore, answering the question such as which factors contribute most to heart disease can be really necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The data set contains around 20 variables, and each observation represents a person's health information. What we are trying to do is to create several machine learning models (SVM, logistic regression, random forest, trees, etc.) to predict whether a patient (using his/her health data) will potentially have heart disease or not. Where in the dataset, the “HeartDisease” binary variable - “Yes” and “No” - represents our predictive Y-values. In details, we are given a X matrix where each row represents a person’s health data, and a Y vector correponding to X matrix that whether that individual has heart disease or not. Our goal is to create machine learning model on the training porportion (both X and Y) of the data so that when given some test data (X test matrix), the model can predict whether each person (one row in X) has heart disease or not. We can also calculate the accuracy, cross-validation, or other evaluation metrics for our model. Besides that, our model should also work even when variable is missing or changed in some parts. Our goal is to create a model with high accuracy at predicting the heard disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "- DataSet link: https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease\n",
    "\n",
    "- There are a total of 320k observations and 18 variables or features. Of these 18 variables, 9 of them are Booleans, 5 of them are string, and 4 of them are numeric.\n",
    "\n",
    "- For a single observation, it includes a comprehensive set of information about an individual, which allows us to predict whether an individual has heart disease with high accuracy. The report consists of sex, age, smoking history, etc.\n",
    "\n",
    "- In all of these features or variables, 3 of them are very critical in determining the accuracy and reliability of our machine learning model. To be specific, age can be regarded as an essential factor influencing the probability of having heart disease, and we use numeric values to represent it. Second, smoking history can be seen as another critical indicator of heart disease. Most patients with heart diseases have a long smoking history. In this dataset, we use Boolean values to represent this information.\n",
    "\n",
    "- Moreover, BMI also plays an essential role in affecting the likelihood of getting heart disease. Many studies have shown the close relationship between them. And we use numeric values (float) to represent it in this dataset.\n",
    "\n",
    "- For the variables (features) that are Boolean or String, we need to convert them into categorical variables using one-hot encoding. For the numeric variables, we may apply some normalization to them, which will help improve the accuracy of our machine learning model. Also, we may need to abandon some features. Precisely, we can calculate the Pearson correlation coefficient for the variables and decide whether to leave it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "Solution K-NN:\n",
    "- Library used: pandas, NumPy, (possible sklearn).\n",
    "- We need to separate the train and test set of the data.\n",
    "- Since our dataset has a nearly 100% valid rate for every column, and there only exist 3 types of variables: Binary(True or False), Classes (Class A, Class B, Class C, etc.), and numeric. Hence we are able to use NumPy to create a one-hot-coding matrix for all variables (or use sklearn to create a one-hot-coding matrix is also a solution). Also, we need to separate the dataset into proportions for cross-validation, and our one-hot-coding matrix then will be separate as well for cross-validation.\n",
    "- Then we can write a function using NumPy to calculate the distance between two points in the one-hot-coding matrix, where the distance is the second norm of (x1-x2).\n",
    "- After that, we can calculate the first “k” closest data points in the training set to different data points in the testing sets, and decide which class the data points in the testing set belong to.\n",
    "Then we evaluate the result and analysis for both cross-validation and the un-trained (test) set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset on Kaggle does not have major ethical, privacy, or terms of use issues. The data of heart disease is collected from the Behavioral Risk Factor Surveillance System, which is a telephone survey system that collects people’s health data. It is part of the Centers for Disease Control and Prevention, which is a government agency, so there might not be much ethical concern or biases about how health data is collected. Since the dataset does not include personally identifiable information, it probably has few privacy issues.\n",
    "\n",
    "In order to protect our dataset from ethical and privacy issues, we will compare data from different sources to verify the data is free of biases and ethical concerns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *1) We are expected to attend weekly meeting and be prepared for meeting.*\n",
    "* *2) Everyone is expected to contribute evenly to this project.*\n",
    "* *3) Everyone's idea will be respected by others.*\n",
    "* *4) Finish one's work one day before deadline so that others could proofread everything.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 4/17  |  10 PM |  Brainstorm topics/datasets (all)  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research; draft project proposal; Edit, finalize and submit proposal | \n",
    "| 4/24  |  10 AM |  Do background research on topic | Discuss ideal dataset(s) and ethics; Discuss possible models to implement | \n",
    "| 5/1  | 10 AM  | Search for datasets (Beckenbaur)  | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part   |\n",
    "| 5/8  | 6 PM  | Import & Wrangle Data ,do some EDA (all) | Review/Edit wrangling/EDA; Discuss Analysis Plan; Finish checkpoint   |\n",
    "| 5/15  | 12 PM  | Finalize wrangling/EDA; Begin programming for project (all) | Discuss/edit project code; Complete project |\n",
    "| 5/22  | 12 PM  | Keep programming for projects and add analysis | Discuss/edit full project |\n",
    "| 5/29  | 8 PM  | Finalize analysis; Draft results/conclusion/discussion (all) | Finalize full project  |\n",
    "| 6/8  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"CDC\"></a>1.[^](#admonish): CDC Heart Disease Facts.https://www.cdc.gov/heartdisease/facts.htm<br>\n",
    "<a name=\"BMC\"></a>2.[^](#admonish): BMC Medical Informatics and Decision Makinghttps://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5<br>\n",
    "<a name=\"Hindawi\"></a>3.[^](#admonish): Prediction of Heart Disease Using a Combination of Machine Learning and Deep Learning https://www.hindawi.com/journals/cin/2021/8387680/<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
